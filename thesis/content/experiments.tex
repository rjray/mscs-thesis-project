\section{Details of the Experiments}
\label{sec:experiments}

To explore these hypotheses a range of experiments were executed in the chosen languages (C, C++, Rust, Perl, Python), with the programs being run under a ``harness'' application that measured various memory, performance, and energy metrics. Additional tools were used to examine the programs for memory leaks, as well as measure aspects of the source code itself.

\subsection{Definitions and Measurements}

To begin, some concepts will be covered and terms will be defined.

\subsubsection{RAPL (Running Average Power Limit)}

The \textit{Running Average Power Limit} (RAPL) measurement system was introduced by Intel into their CPU products starting with the Sandybridge family of processors. The system allows measuring energy over several areas:

\begin{description}
\item[Package:] The full (socketed) processor package, which may contain multiple cores.
\item[Power-Plane 0:] The domain that encompasses the combined cores within the package. This reading will cover all cores within the CPU of the package.
\item[Power-Plane 1:] Sometimes referred to as ``uncore'', this domain generally covers the integrated GPU (if present).
\item[DRAM:] The domain for the system's DRAM, whose energy usage is separate from the package.
\item[Psys:] The domain that covers the entire system-on-chip energy usage. This would include the package and DRAM values, as well as other system-level energy consumption.
\end{description}

A computer system may have more than one package, and the RAPL interface includes methods for determining the number of packages and gathering the energy readings for each package separately. However, it is not possible to measure a package's energy usage at the level of an individual core.

Reading the RAPL data is done through the \textit{model-specific registers} (MSR) interface, as detailed in~\cite[Chapter~14]{intel.sdm.2022}. The method involves reading several registers to determine the number of packages the system has, then determining the scaling factors for each of time units (expressed in seconds), power units (Watts), and energy units (Joules). The scaling factors are stored in a single register referred to as \texttt{MSR\_RAPL\_POWER\_UNIT}, as groups of bits within the register. Each scaling factor is a 4-bit (5-bit in the case of energy units) value used to compute a fractional floating-point number (where $b$ represents the value of the n-bit factor):

\[S~=~\frac{1}{2^{b}}\]

In the case of the energy units factor, the value of $b$ on the test platform was 01110b (14), and $S~=~61.04~\mu$J.

Obtaining the data during run-time from RAPL requires reading from of a series of read-only registers within the MSR and scaling the values obtained by the appropriate $S$ for the units. For example, reading the Power-Plane 0 (CPU) energy value uses the \texttt{MSR\_PP0\_ENERGY\_STATUS} register. The value obtained is 64 bits in width, though only the initial 32 bits hold energy data (the high 32 bits are reserved by Intel). The value read is masked to remove any high bits, then multiplied by the energy units scale factor to produce a value in Joules.

An issue with the RAPL system was encountered during runs of the experiments: because the value of the energy registers is 32 bits in size, it wraps around to 0 when the maximum value is reached. This caused occasional anomalous readings in cases where the register would reset between the initial reading and the final reading, resulting in a negative overall value. The program that processes the output from the experiments was adjusted to recognize such values, discard them from consideration, and produce appropriate notes when a set of samples was thus shorter than the rest.

\subsubsection{SLOC (Source Lines Of Code)}

The SLOC, or \textit{Source Lines Of Code}, measurement attempts to evaluate the conciseness of the source code to a program. It generally distinguishes between physical lines of text, comments, and actual source lines.

As a metric of code quality or developer productivity, SLOC is not without some controversy. In~\cite{alpernas.2020} the authors point out that measuring lines of code can be very diverse in its execution, and often not clear in its purpose. Here, the purpose of measuring SLOC will be simple: comparison of the implementation of identical algorithms in different languages.

\subsubsection{Metrics}

For each program comprising an experiment, the following data was gathered either dynamically (from executions of the program) or statically (through non-execution analysis of program source code).

\begin{description}
\item[Total Program Run-time:] The complete run-time of the program, as measured by the harness program. Unlike the next metric, this will include program initialization time, the input/output operations of loading the data to be processed, etc. Measured in floating-point seconds.
\item[Algorithm Run-Time:] The time spent within running the algorithm itself over the complete set of test data. This is measured solely on the processing of data, and does not include I/O, set-up of the environment, or post-algorithm steps such as freeing of memory. Also measured in floating-point seconds.
\item[Maximum Memory Usage:] The largest amount of memory allocated for the running program throughout the course of its execution, in megabytes. This represents the largest size to which the program grew during the run.
\item[Power-Plane 0 (CPU) Energy Usage:] The energy consumed by the CPU cores during the execution of the program. Measured over the full lifespan of the program, not just the algorithm itself. Measured in Joules.
\item[DRAM Energy Usage:] The energy consumed by the DRAM during the full lifespan of the program. Measured in Joules.
\item[Full Package Energy Usage:] The energy consumed by the full (socketed) package, which includes the CPU cores' energy but not the DRAM energy. Also measured in Joules.
\item[Source Lines Of Code:] The measured lines of code in the implementation of the program, using a tool\footnote{sloc: \texttt{https://github.com/flosse/sloc}} designed to measure these values using consistent standards across the different programming languages.
\end{description}

The Power-Plane 1 (GPU) RAPL values were excluded because the testing machine's package did not have an integrated GPU. Additionally, the Psys values were excluded because they were deemed unnecessary in the context of having the package, CPU and DRAM values.

\subsection{Languages Under Consideration}

The experiments were performed on five languages. The languages were chosen for their commonalities as well as their differences:

\begin{itemize}
\item Three of the languages (C, C++, Rust) are compiled to machine code and were chosen for performance first and foremost. The remaining two (Perl and Python) are interpreted (``scripting'') languages which are highly regarded for speed of development and rapid prototyping, as well as being popular in bioinformatics computing.
\item Each language is currently in widespread use across different disciplines of software development.
\item The languages have differing aspects in their approach to memory management, as are detailed below.
\end{itemize}

\subsubsection{C}

The C programming language is the oldest and most-established of the languages. Originally designed in the early 1970's by Dennis Ritchie, it remains a very widely-used and influential language since its first appearance in 1972. Since 1989, it has been standardized by both ANSI (the American National Standards Institute) and by the International Organization for Standardization (ISO).

C relies on manual memory management, meaning that the programmer is responsible for all allocation and freeing of dynamic memory. This approach can often lead to several major classes of bugs when used incorrectly, such as memory safety issues or memory leaks. Multiple pointers to the same region of memory can become ``dangling pointers'' when one frees the memory without the other pointers being invalidated at the same time. Further attempts to use any of the other pointers can lead to memory corruption or segmentation faults.

\subsubsection{C++}

C++ was developed initially as an extension of C, by Bjarne Stroustrup while working at AT\&T Bell Labs. It first appeared in 1985 and was initially standardized in 1998. At first envisioned as ``C with Classes'', the language has been significantly expanded over the years to include many more features while still maintaining low-level memory accessibility. C++ attempts to offer more expressive, concise coding than C, with many of C's memory-management concerns dealt with automatically by class constructors and destructors.

\subsubsection{Perl}

\subsubsection{Python}

\subsubsection{Rust}

Rust is the newest of the languages, having first appeared in 2010. Rust offers a promise of expressiveness equal to or greater than C++, with greater safety in the areas of memory management and ownership. It is a multi-paradigm, general-purpose language that draws from several previous languages including C++, Haskell, and Standard ML. While often referred to as a systems programming language, its usage is spreading rapidly to other areas including to some scientific programming disciplines~\cite{nature.rust.2020}. The language began in 2006 as a personal project of Graydon Hoare, an employee of the Mozilla Corporation, with Mozilla beginning to sponsor the work in 2009 and officially announcing the project in 2010~\cite{asay.2021}. The first pre-alpha numbered version of the compiler was Rust 0.1, which was released in January of 2012. The current (as of this writing) version of Rust is 1.64.0 and was released in September of 2022.

An area where Rust is distinct from other C-based languages is in the way it manages memory and tracks values on the stack and heap. Rust uses an ownership system~\cite[Chapter~4]{programming.rust.2021}, with the ability to specify lifetime information for reference types. There is no automated garbage collection, and resources are managed through the same convention of \textit{resource acquisition is initialization}~\cite{cpp.design.evolution.1994} as in C++, with optional reference counting. Rust's design for memory safety does not permit null pointers, dangling pointers, or data races.

With languages such as C and C++, \textit{data ownership} is handled largely through practice and convention. An instance of a C++ \texttt{std::string} owns the buffer allocated for the storage of the string data. Other variables may be created, though, that point to the same buffer or a single character within it. These other interests in the content of the string buffer have their own responsibility for noticing when the original string object is destroyed and the buffer freed. After such point, the outside interests are each responsible for marking their references as no longer valid.

In contrast, Rust integrates the concept of ownership directly into the language itself. Compile-time checks enforce ownership and report violations. When the owner of a value is ``dropped'' (Rust terminology for freeing) the owned value is dropped as well. While the variables themselves are on the stack, the content is allocated on the heap. Variables own their values, and the complex datatypes (structs, tuples, arrays, and vectors) own their elements.

\subsection{General Implementation of the Algorithms}

Each of the algorithms is implemented according to consistent structure, to better facilitate the direct comparison of the source code across languages. This structure consists of three basic elements:

\begin{itemize}
\item An ``input'' module that encapsulates the loading of sequence, pattern, and answer data
\item A ``runner'' module that provides the main-loop of the program
\item An ``algorithm'' module that provides the code specific to the algorithm being used as a basis for the experiment
\end{itemize}

\subsubsection{Input modules}

The input modules allow the main-loop modules (described next) to further abstract the reading of the external data used in each experiment. Data is separated into three files: the \textit{sequences} file contains lines of randomly-generated target strings, the \textit{patterns} file contains the crafted patterns to search for within the sequences, and the \textit{answers} file provides a representation of the correct number of times each pattern should be found in each sequence. This allows the runner modules to verify the results of each invocation of the algorithms being evaluated. The nature of the data and its creation is further detailed in~\ref{subsec:data}.

Each input module defines three routines, one for each of the data files. In most cases, the reading of the pattern files was essentially identical to the sequence files and thus the pattern routine simply calls the sequence routine.

The input modules are the first place in which the distinction in expressiveness and style between the languages becomes apparent. Differences become immediately visible in just the comparison of the C and C++ implementations, where the physical combined length of files (in C and C++, the input modules also required accompanying header files) differs by over 40\% in favor of C++. Python measures as being just over 20\% of the size of the C code.

\subsubsection{Runner modules}

Each runner module utilizes the input module to read in the experiment data and loop over it. In the single-pattern algorithms, this is a nesting of two loops: the outer loop iterates over the set of pattern strings and the inner loop iterates over the set of sequence strings. Each iteration of the loop over the sequence strings triggers one execution of the algorithm being evaluated. In the multi-pattern algorithms, this is a single loop over the set of sequence strings, as the complete set of patterns are pre-processed prior to the loop.

The runner records the time (by system wall-clock) when the algorithm pre-processing begins, and the time when all loops and answer validation has completed. Everything that is not input-related or related to reporting of results is recorded in this span of time. At this point, the runner prints three lines to the standard-output stream. The lines identify the language (including compiler variants for C and C++), the algorithm, and list the time spent in the main loop. The runner is also responsible for handling the arguments passed to the program as well as determining the exit-code of the program (to allow the harness program to discern failing runs from successful runs).

\subsubsection{Algorithm modules}

The algorithm modules are the heart of the experiments. To maintain consistency, each algorithm module defines a minimum of two functions: an initialization routine and the primary algorithm entry point.

The initialization routine is responsible for any pre-processing necessary for the pattern string, and produces a collection of data elements that represent the pattern in the appropriate internal structure. The exact nature and structure of this representation is language-dependent.

The algorithm entry point routine is the means by which the each algorithm was applied to the pattern and sequence under consideration. It receives the pattern representation produced by the initialization routine and the sequence representation as parameters, and returns a numerical value indicating how many times the pattern was successfully found within the sequence. In the case of the multi-pattern algorithm implementations, the return value from this routine is a vector of numbers with length equal to the number of patterns.

In addition to these two functions, each algorithm module defines all needed support code for the initialization and entry point. In some cases (such as the C and Rust implementations of the Aho-Corasick algorithm) this included minimal implementations of data structures such as sets and simple queues.

Each algorithm module also provides the language-specific equivalent of a ``main'' function, that function which is treated as the program entry-point by the operating system. Each ``main'' function consists of a single call to the runner function provided by the runner module. The call passes the two algorithm-specific functions as pointers (again, in a language-specific manner) to the runner, followed by the name of the algorithm and a representation of the command-line arguments.

\subsection{Selected Algorithms}

The different algorithms that were chosen are listed here in the order of their implementation. Each of the algorithms was implemented first in C, as a baseline. These C implementations were then used as templates for the other languages' implementations. In addition to helping to keep the implementations approximately similar and equal, this provided insight into the difference that the features and expressiveness of the different languages made in the development process itself.

\subsubsection{Knuth, Morris, and Pratt}

The Knuth-Morris-Pratt~\cite{knuth.morris.pratt.1977} algorithm is one of the foundational algorithms in the area of string matching and text searching. It is still used in the present day, with implementations as recent as the Rust-Bio project~\cite{rust.bio.2015}. This was chosen primarily for historical significance, but also for ease of implementation.

Knuth-Morris-Pratt is an \textit{exact matching} algorithm, meaning that it matches the desired pattern exactly or not at all. It finds all instances of the pattern within the target string, including overlapping instances, in time linear to the sum of the pattern length and the target length.

Generally when matching, the pattern is aligned with positions in the target string. Shifting is done when a character mismatch is found between an index in the target and the corresponding index in the pattern. Where a naive implementation might shift the pattern by one place after each failed match, resulting in a time complexity approaching O($mn$), the Knuth-Morris-Pratt approach is built on the concept of an auxiliary table (referred to as the ``next'' table) that instructs the matching algorithm on how much to shift the pattern over the target stream. Based on repetition within the pattern itself, the table may call for a shift of the pattern by more than one character for a given mismatch.

The computation of this table is shown in the paper to require O($m$) steps, and the process of matching the pattern to the target takes at most an additional $2n$ steps. This is due to the fact that, at each step of the matching process, only one of the text pointer or the pattern pointer are moved (each of which can only move $n$ times at most). This results in a worst-case run-time bounded by O($m+n$).

As an example, consider a search for the pattern ``CTAGC'' in a sequence that starts with ``CGCCTAGCG''. The first step is to compute the ``next'' table according to the algorithm, shown in figure~\ref{fig:kmp_next}.

\begin{figure}[ht]
\centering
\input{figures/kmp-next}
\caption{Knuth-Morris-Pratt next-table}
\label{fig:kmp_next}
\end{figure}

With the table in hand, the process moves to matching. The matching algorithm goes through the following steps:

\begin{enumerate}
\item $i$ and $j$ both initialize to 0
\item $p_0 = s_0$, so $next$ is not consulted
\item $i$ and $j$ increment, both to 1 and 1
\item $p_1 \neq s_1$, so $i = next[1]$ and becomes 0
\item $p_0 \neq s_1$, so $i = next[0]$ and becomes -1
\item $i$ and $j$ increment, to 0 and 2 respectively
\item $p_0 = s_2$, so $next$ is not consulted
\item $i$ and $j$ increment
\item $p_1 \neq s_3$, so $i = next[1]$ and becomes 0
\item $p_0 = s_3$, so $next$ is not consulted
\item $i$ and $j$ increment
\item $p_i$ continues to match $s_j$ as both variables increment. When $i = 5$, the algorithm detects a match.
\end{enumerate}

The C implementation of Knuth-Morris-Pratt was adapted from~\cite[Chapter 7]{handbook.2004}. Of note is the discovery that the sample code takes advantage of C's use of a ``null'' byte at the end of a string to stand in for the sentinel character that the algorithm appends to the pattern string. While this optimization was also applicable to C++ (where strings are represented by instances of the \texttt{std::string} class), each of the three remaining languages were required to manually add the sentinel character to the pattern string prior to computing the ``next'' table.

This being the first of the algorithms implemented, it is also where the means of passing generic data from the initialization routines to the algorithm routines was developed. The differences in these methods, particularly between the compiled languages, spoke strongly to their relative expressiveness.

In the C implementation it was necessary to use a memory pointer of the type, \texttt{void **}. This defines a dynamic list of dynamic pointers, but provides no information about each individual pointer. It is left to the code to properly type-cast the values from this block of memory. If the program is incorrect (such as getting the order of elements backwards) the resulting typed pointers will likely trigger memory faults when dereferenced. This was also the most-flexible of the compiled solutions and required no difference in implementation between the different algorithms.

In C++ it was initially attempted to reuse the \texttt{void **} approach that had been used in C. This proved to be extremely difficult under the stricter compiler, and the decision was made to use the \texttt{std::variant} class instead, which represents a type-safe union in the language. While the same variant type definition could have been used for both the single-pattern and multi-pattern algorithms, it was decided to distinguish them for the sake of readability. The type definitions were made in the file \texttt{run.hpp}, the header-file for the runner module of C++. Unpacking a vector of the variant types brought further type-safety in the form of a generically-typed function \texttt{get<>} in the standard library that was used to extract the values to new objects of the correct type.

Rust had the language features that seemed the most clear and expressive in implementation: enumeration and pattern-matching. In Rust, an enumeration type (\texttt{enum}) can optionally assign data types to the elements of the enumeration. When an enumeration element is instantiated it is provided an instance of that data when applicable. This lead to a system for the Rust programs in which the initialization function for an algorithm created and returned a vector of the enumeration type. The algorithm routines that received these types would extract them from the vector using a pattern-matching\footnote{In the computer language sense, which is distinct from pattern-matching in strings} construct that ensured the types of each individual element of the vector received from the initialization function. The program would raise a run-time exception if the given vector element was not of the expected type. In contrast, Rust required the least-expressive approach to appending the sentinel value onto the pattern string for the sake of creating the ``next'' table.

Given the dynamically-typed nature of data in Perl and Python, both of these implementations were simple lists of values returned without any extra effort at encoding. As with the \texttt{void **} approach the onus was on the program to unpack the elements in the correct order. Trying to use a list or list-reference value as an ordinary scalar would have caused a run-time exception in either program.

\subsubsection{Boyer and Moore}

In the same year that Knuth, Morris and Pratt published their paper, Boyer and Moore published as well~\cite{boyer.moore.1977}. This algorithm is also an exact matching approach, that is based on the research of Knuth, et al. The Boyer-Moore performance improvements are based on searching from the end of the pattern rather than the beginning, and computing two tables to use in optimizing the jumps through the sequence string when mismatches are discovered. This algorithm was chosen as an example of refinement and improvement of another sample algorithm.

Boyer and Moore postulated that, ``more information is gained by matching the pattern from the right than from the left.'' For example: If the target character that corresponds to the current location of the last character from the pattern is not only a mismatch but also does not appear in the pattern at all, the pattern may then be shifted right by its full length. They refer to this algorithm as being ``usually sublinear,'' meaning that when finding the location of the pattern within the target the number of compared characters is usually less than $i+m-1$ (where $m$ is the pattern length and $i$ is the position within the target where the match of the pattern begins).

The first of the two tables is the simplest to compute. It is the size of the alphabet of the pattern and sequence\footnote{In these implementations, to avoid constantly translating the four characters into values between 0 and 3, the alphabet-size was set to 128 for convenience.}, and it tracks the number of positions by which the pattern can be moved down the sequence without additional checking for matches. Boyer and Moore define each entry in this table as being $m$ (the pattern's length) when the character does not appear in the pattern at all, and $m - i$ otherwise (where $i$ is the right-most index within the pattern where the character does occur). Using the same example pattern and sequence as in the previous section, the first table (referred to as $delta_1$ in the paper and $bad\_char$ in the implementations) is shown in figure~\ref{fig:bm_bad_char}. Only the entries for the four characters that appear in patterns are shown.

\begin{figure}[ht]
\centering
\input{figures/bm-bad-char}
\caption{Boyer-Moore $delta_1$ table}
\label{fig:bm_bad_char}
\end{figure}

The second table (referred to as $delta_2$ in the paper and $good\_suffix$ in the implementations) is more complex to calculate, as it first requires calculation of suffixes within the pattern. In the paper, it is described as the distance that the pattern can be slid down in order to align the discovered sub-match (in the target) with the last $m - j$ characters of the pattern (where $j$ is the index within $delta_2$), plus the additional distance that the pointer within the target must be moved so as to restart the matching process at the right end of the pattern. This table is shown in figure~\ref{fig:bm_good_suffix}.

\begin{figure}[ht]
\centering
\input{figures/bm-good-suffix}
\caption{Boyer-Moore $delta_2$ table}
\label{fig:bm_good_suffix}
\end{figure}

When using the two tables to select the amount to shift, it is possible that the $delta_1$ table's value may be negative. Because of this a $max$ operator is applied to the two potential values and the largest possible shift is chosen. With both tables computed, the matching process begins with the pattern aligned to the starting character of the target string. The algorithm then goes through the following steps (where $m$ is the pattern length and $n$ is the sequence length):

\begin{enumerate}
\item $j$ (the pointer within the sequence) initializes to 0
\item $i$ (the pointer within the pattern) is set to $m - 1$ (4)
\item $p_4 \neq s_4$, so the tables are consulted
\item $delta_2(4)$ is 1, and $(delta_1(T) - m + 1 + i)$ = 3
\item $j$ advances by 3
\item $i$ is set to $m - 1$ (4)
\item $p_4 = s_7$, so $i$ is decremented
\item $p_i$ continues to match $s_{i+j}$ until $i$ becomes -1
\item A match is recorded and $j$ advances by $delta_2(0)$, to 7
\end{enumerate}

While the Knuth-Morris-Pratt algorithm had made 11 character comparisons to find the match, Boyer-Moore makes only 6, 5 of which were required to verify the match.

The C version of the basic algorithm was drawn from~\cite[Chapter 14]{handbook.2004}. Like Knuth-Morris-Pratt, it requires a character at the end of the pattern for calculating the suffixes table. The C and C++ implementations use the ``null'' string-terminating byte for this while the others add it explicitly.

Starting with Rust in this case, one aspect became very clear in this algorithm's implementation: the need to regularly cast various numerical (particularly integer) types to other types. Because of the larger amount of array-indexing in this algorithm, the number of times an \texttt{i32} (signed 32-bit integer) or \texttt{u32} (unsigned) had to be cast to or from a \texttt{usize} type (an unsigned type used for lengths and indexing) was significant. While this does lead to fewer bugs in the code, at the same time it detracted from the conciseness of the Rust version.

Perl and Python were comparable in their conciseness and logical expression of the algorithm. Python showed an advantage in the form of its native support for iterators as a type and functions such as \texttt{range} that return these iterators. Operations like looping backwards through a series of integers was clearer in the Python code than in the corresponding Perl.

The C and C++ implementations are best compared to each other, as this is another case where the C++ Standard Library classes lead to clearer code. Though there was less difference in code-length, various mechanisms of C++ in areas such as argument-passing meant less overhead and less ambiguity about pointers sent and received.

\subsubsection{Bitap}

The Bitap algorithm (sometimes known as ``Shift-Or'', or ``Shift-Add'') was initially developed by B\'{a}lint D\"{o}m\"{o}lki in 1964. In 1989 it was re-invented by Ricardo Baeza-Yates and Gaston H. Gonnet, and published in~\cite{baeza.yates.gonnet.1992} in 1992. Here, it has been chosen for the distinctive approach when compared to the other algorithms.

In terms of time complexity, the algorithm is on the same level as the previous ones, having a preprocessing time of O($m+\sigma$) (the length of the pattern plus the size of the alphabet) and a running time of O($n$). However, the operations it performs are all bit-oriented: shifts, complements, bitwise-and and bitwise-or. This resulted in this algorithm running significantly faster than either of Knuth-Morris-Pratt or Boyer-Moore on the same input data.

The structure of the algorithm is based on encoding the pattern in a vector of bitmaps. The vector has length equal to the alphabet size, and each element of the vector is a bit-field of width $W$, where $W$ is the size in bits of an unsigned integer. $W$ also limits the length of the pattern in this implementation, though it is possible to implement the algorithm for longer patterns.

The vector is initialized in the preprocessing phase first to an all-1's value in each space, and then modified through a single pass over the pattern. For each character in the pattern, the vector space corresponding to that character has the bit that corresponds to the position in the pattern flipped to 0. The resulting vector fully encodes the pattern. Using the example pattern from the previous two algorithms, imagine that the elements of the vector ($S$) are exactly as wide as the pattern (5 bits) and that there are only the four elements corresponding to the restricted alphabet of the pattern. This is shown in figure~\ref{fig:bitap_s_positions}.

\begin{figure}[ht]
\centering
\input{figures/bitap-s-positions}
\caption{Bitap shift positions vector}
\label{fig:bitap_s_positions}
\end{figure}

During the process of computing the $S$ vector, a ``limit'' value is also calculated: it starts out as 0, and has a number of bits set equal to the full with of the pattern. At the end of the loop that calculates $S$, the value of $limit$ is shifted to the right by one bit, then subject to a bit-complement operation. The result is a value that can be compared with, when determining if a match has been found.

The process of searching for a match begins with a $state$ value of $W$ bit-width, set to all 1's. The target string is read one character at a time. For each iteration of this loop, $state$ is shifted to the left one bit (introducing a 0) and then \texttt{or}'d with the $S$ value of the character under the index. If, after this operation (the ``shift-or''), the value of $state$ is less than the value of $limit$ then a match has been found and will be reported.

\begin{figure}[ht]
\centering
\input{figures/bitap-matching}
\caption{Bitap matching process}
\label{fig:bitap_matching}
\end{figure}

In figure~\ref{fig:bitap_matching}, the $state$ column shows the value of that variable at each iteration, while the ``result'' column shows the value of $state$ after the \texttt{or}-operation. On the row where $j = 7$, we see that the high bit of $state$ is a 0 for the first time. This signals a match, and the matching index is $j - m + 1$, or 3.

The C version of Bitap was taken from~\cite[Chapter 5]{handbook.2004}, where it is referred to as Shift-Or. The design of the algorithm given there is clearly drawn from the Shift-Or algorithm given in~\cite{baeza.yates.gonnet.1992}. The C++ version followed the C very closely with the only changes being the use of standard classes for strings and vectors. Likewise, the Rust version of this algorithm proved very simple and bore a fair resemblance to the C and C++.

The Perl and Python implementations were also very similar to the compiled versions, with the same exception that each language has arrays as first-class data types. The Python version, however, did have one notable difference: due to Python's integers being of arbitrary size, it was not enough to take the bit-complement of 0 to get a ``full'' bit-field. In Python this resulted in a value of -1, rather than the expected $2^W - 1$. To force Python to treat these values as unsigned 64-bit, it was necessary to declare a ``mask'' value equal to $2^W - 1$ and apply this to the result of every logical bit-operation performed. This incurred a performance penalty on the Python implementation, making this the only algorithm to be outperformed by its Perl counterpart.

\subsubsection{Aho and Corasick}

Alfred V. Aho and Margaret J. Corasick published their algorithm for searching multiple patterns at once in 1975~\cite{aho.corasick.1975}. In their algorithm, a finite number of patterns are merged into a single deterministic finite automata (DFA) which can then be applied over any number of target strings. The DFA is capable of finding all locations of all patterns in the set, with overlapping, in a single pass through the target string. This makes the algorithm considerably faster than the others for the simple reason that Aho-Corasick scans each target string once, regardless of the number of patterns. By contrast, each of the previous algorithms would scan the target once per pattern. The choice of this algorithm was driven by an interest in comparing a multi-pattern algorithm to the single-pattern options, and an interest in using a DFA for the matching process.

Aho and Corasick designed the pattern matching machine as a collection of three functions:

\begin{itemize}
\item A ``goto'' function $g$, which maps transitions from one state to the next based on the character being examined
\item A ``failure'' function $f$, which maps a state into another state whenever $g$ reports that there is no transition for the current state and current character
\item An ``output'' function $output$, which maps states to sets of patterns matched at the specific state
\end{itemize}

Construction of these three functions is accomplished through two supporting algorithms. The first fully constructs $g$ and partially computes $output$. The second fully constructs $f$ and completes the computation of $output$. The first algorithm takes only the set of patterns (referred to there as $K$) as input, while the second algorithm takes the resulting $g$ and $output$ from the first as input.

For an example, let $K$ be the list $\left\lbrace ACTG,~CTG,~AAGT \right\rbrace$. Constructing $g$ by the algorithm given in the paper yields the DFA given by the figures~\ref{fig:ac_goto_function} through~\ref{fig:ac_output_function}.

\begin{figure}[ht]
\centering
\input{figures/ac-goto-function}
\caption{Aho-Corasick goto function}
\label{fig:ac_goto_function}
\end{figure}

Starting with figure~\ref{fig:ac_goto_function}, the goto function $g$ provides the finite-state machine that lays out the transitions between states based on the character under consideration. Any state that does not have a transition for a given character is assumed to ``fail'' on that character, which is where the failure function $f$ comes into use. While it is given in the algorithm that there are no failure transitions for state 0, this is made explicit in the figure by including a self-referencing transition for ``G'' and ``T''.

\begin{figure}[ht]
\centering
\input{figures/ac-failure-function}
\caption{Aho-Corasick failure function}
\label{fig:ac_failure_function}
\end{figure}

In figure~\ref{fig:ac_failure_function}, the failure function $f$ shows how the processing of the DFA may jump around to implement a form of back-tracking. State 2 will move to state 5 on a failure transition, reflecting that the character previous to the state (``C'') could instead be the start of matching the ``CTG'' pattern. Because there are no failing transitions in state 0, that state is not represented in the figure.

\begin{figure}[ht]
\centering
\input{figures/ac-output-function}
\caption{Aho-Corasick output function}
\label{fig:ac_output_function}
\end{figure}

In figure~\ref{fig:ac_output_function}, the $output$ function is represented as an indexed array of sets. Most states (including state 0) have the empty set as output. The non-empty sets represent the found patterns by their index in the original list. State 10 has only a value of 2, representing the pattern ``AAGT''. Of note is state 4, whose set includes both the index 0 and the index 1. This reflects the fact that the three characters of pattern 1 overlap the last three characters of pattern 0.

To illustrate the process of this algorithm, consider the target string that begins with the sequence ``AACTG...''. Figure~\ref{fig:ac_progression} shows the progression of states as the pointer advances through the first five characters.

\begin{figure}[ht]
\centering
\input{figures/ac-progression}
\caption{Example of Aho-Corasick state progression}
\label{fig:ac_progression}
\end{figure}

Starting at the 0 state, the first ``A'' moves the DFA to state 1 after which the second ``A'' moves it to state 8. From 8, the character ``C'' is failure transition, so the value of $f(8)$ is read. The value is 1, so the DFA immediately moves to state 1 and looks for a transition on ``C''. It exists, and moves the DFA to states 2, 3 and 4 in turn. Upon reaching state 4 the value of $output(4)$ is checked and found to not be the empty set, so the machine signals a match of the patterns ``ACTG'' and ``CTG''.

For the implementation of this algorithm, no existing code was consulted. Rather, the initial C version was developed directly from the algorithm specification in the paper itself. The C version required the most supporting code, as the algorithm calls for both queue and set data structures. Additionally, the construction of the $g$ and $output$ functions required dynamic re-sizing of lists. This was avoided by generously estimating the size needed for those lists and pre-allocating that amount. Constructing $f$ did not have this problem as the size of $g$ was known at that point. Both queue and set data structures were implemented with just the minimally-required functionality for each. For the queue, this was the operations \textit{create}, \textit{delete}, \textit{expand} (grow), \textit{enqueue}, \textit{dequeue}, and a test for whether the queue is empty. For the set, these were \textit{create}, \textit{delete}, \textit{expand} (grow), \textit{insert}, \textit{test-for-membership}, and an implementation of a \textit{union} operation. These implementations added significantly to the number of dynamic memory allocations and corresponding releases.

When implementing in C++, the C++ Standard Library was able to provide existing classes for both set and queue implementations, greatly reducing the amount of supporting code in this version. Though C++ vector types can dynamically grow, intermediate experiments found that relying on this for the $g$ and $output$ function construction introduced a slight performance penalty. As such, it was decided to replicate the C approach of estimating their size and pre-allocating. Using C++ features, this also allowed for immediate initialization of the vectors where the C arrays required explicit loops to initialize. In the end, the difference between the two languages' implementations was a factor of almost 2x for the C code over the C++ code.

The Perl and Python implementations were both significantly shorter in length than the C or C++, due to the native support for resizable lists. For the queue data structure, the Python code used the \texttt{collections.deque} class that is part of the Python core. Perl used an ordinary list, as the language provides a built-in keyword for removing from the head of a list. Both languages grew the $g$ and $output$ functions dynamically rather than pre-allocating them.

The Rust implementation went through several iterations before reaching its final form. The first version closely followed the C++ version: the use of resizable vectors through the native \texttt{Vec<>} type, and implementations of set and queue from the standard Rust library. This initial version severely under-performed in comparison to the other two compiled solutions. With help from the Rust community, it was determined that the two collection classes were incurring large amounts of overhead. To address this, they were replaced with simple implementations adapted from the C code. As with the Perl and Python versions, the \texttt{Vec<>} type was allowed to handle the dynamic natures of the $g$ and $output$ functions. As with C++, some intermediate experiments were performed that pre-allocated these vectors based on estimates of needed space. But here also it was found that at best the performance remained the same, and in some runs was noticeably worse.

\subsubsection{Other Algorithms Considered}

\subsection{Optimizations}

Over the course of the development of the experiments code in each language, the need became clear for some basic optimizations. In every case, any optimization was applied consistently across all languages. Some examples of optimizations include:

\begin{description}
\item[Data Preprocessing:] While the C and C++ languages were able to seamlessly use individual characters from the strings as array indices, Rust and the scripting languages were not. Based on an assumption that production-targeted code would apply any similar preprocessing, the strings were converted to forms directly usable by the other languages. In the case of Rust, this was a conversion of strings to arrays of unsigned 8-bit integers. In the case of Python, it was a direct mapping of strings (which are already treated as sequences by Python) into the ordinal values  of each character. Perl required the most preprocessing, with strings first being converted to arrays of individual characters before being mapped to their ordinal values.
\item[Pattern Preprocessing:] To bring down the running times of the script languages' longer experiments, a mechanism for preprocessing patterns was developed that allowed for each pattern to be processed only once prior to being applied to all sequences. Without this, some instances of the Knuth-Morris-Pratt algorithm took close to an hour to complete.
\item[Minimizing Type-Casting:] In the case of Rust's strong typing, it was necessary to frequently cast integer values into Rust's \texttt{usize} type for use as array indices. Though this would have had very little effect on run-time, the decision was made in most cases to declare a cast version of the integer value to help in the readability of the code.
\end{description}

All optimization steps were done within the timing bracket of the runner module, and contributed to the overall reported run-time.

\subsection{Data Used}
\label{subsec:data}

\subsubsection{Method of Generation}

\subsubsection{Nature of the Data Used}

\subsection{Testing Platform}

The experiments were run on a dedicated machine running a version of the Linux operating system. The set of installed software was kept minimized to reduce the interference with general energy usage during the runs of experiments.

\subsubsection{Hardware Specifications}

The machine used for the experiments is an Intel-brand NUC7i5BNH i5-7260U, an ultra-compact device referred to by Intel as a ``Next Unit of Computing'' (NUC). The machine features the following specifications:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|ll|}
\hline
Processor & Intel® Core™ i5-7260U Processor\\
Processor Base Frequency & 2.20 GHz\\
Max Turbo Frequency & 3.40 GHz\\
Memory Type & DDR4-2133 1.2V SO-DIMM\\
Installed Memory & 8 GB\\
Internal Drive Form Factor & M.2 and 2.5''\\
Installed Storage & 120 GB M.2 SSD\\
\hline
\end{tabular}
\caption{Hardware specifications of the test platform}
\end{center}
\label{table:hardware_specs}
\end{table}

The CPU is dual-core, with hyperthreading cores, for a total of 4 computational cores. This did not come into consideration for the experiments as none of the code was written to be multi-threaded.

\subsubsection{Operating System and Configuration}

The NUC was cleaned of the vendor-installed operating system. Linux was installed on it using the minimal server edition of Ubuntu Linux 22.04.1. All unnecessary packages and software services were either disabled or removed completely.

The development software (see next section) was then installed. This included the Linux version of the ``Homebrew''\footnote{Homebrew: \texttt{https://brew.sh/}} package manager. Homebrew was specifically used to install the latest version of the LLVM Compiler Infrastructure, to allow all experiment code to be compiled directly on the NUC (rather than copying executable files built on a different machine). The Rust language toolchain was installed and managed using the ``Rustup''\footnote{rustup: \texttt{https://rustup.rs/}} management software. Core software development packages from Ubuntu were also installed at this time.

\subsubsection{Compilers and Other Tools}

The following table lists the primary software tools and packages that were used in the creation and execution of the experiments, with the source from which they were obtained. The double line after the Python details indicates that the remaining tools were used on the development machine only, not on the NUC. These tools were used in the testing and evaluation of the experiments code prior to running experiments on the NUC.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|r|l|}
\hline
\textbf{Tool} & \textbf{Version} & \textbf{Source}\\
\hline
GNU Make & 4.3 & Ubuntu 22.04\\
GCC & 11.2.0 & Ubuntu 22.04\\
LLVM & 15.0.1 & Linux Homebrew\\
Intel\textregistered~oneAPI Compiler & 2022.2.0 & Intel website\\
Rust & 1.64.0 & Rustup Manager\\
Perl & 5.34.0 & Ubuntu 22.04\\
Python & 3.10.6 & Ubuntu 22.04\\
\hline
\hline
Valgrind & 3.19.0 & Linux Homebrew\\
\texttt{sloc} & 0.2.1 & Linux Homebrew\\
\texttt{perf} & 5.15.53 & Ubuntu 22.04\\
\hline
\end{tabular}
\caption{List of software tools used}
\end{center}
\label{table:tools}
\end{table}

The tools used for the experiments themselves include the GNU Make tool, as it was the driver for automating the running of the full range of experiments on a regular basis.

The Valgrind\footnote{Valgrind: \texttt{https://valgrind.org/}} tool was used to identify memory leaks in each of the experiment programs, while \texttt{sloc} was used to measure the Source Lines Of Code metric as part of determining the conciseness and expressiveness of the programs. The \texttt{perf} tool was used to identify performance bottlenecks in the running programs. Lastly, the Python programming language was used not only for experiments but also to develop the tools which process and analyze the data gathered during the running of the experiments, as well as generate the randomized data that the experiments used to exercise the algorithms.
