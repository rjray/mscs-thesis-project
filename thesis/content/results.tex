\section{Results and Analysis}
\label{sec:results}

During the experiment programs' development, an automation mechanism was developed based on the GNU Make tool. This mechanism allowed for the repeated running of the experiments suite while also providing control over aspects such as the number of repetitions and grouping of the algorithms.

The basis for the mechanism was a series of additional rules added to the already-present ``Makefile'' files that had been developed for the building of each language's set of programs. A single target-rule, ``\texttt{experiments}'', would recursively descend into each language-specific directory and trigger all algorithms in sequence. Each algorithm ran a specified number of times, and in most cases the initial run was discarded. This was to prevent the possibility of the data input files being in the disk device's cache, skewing the timing and energy readings with regards to later runs. All non-error output from the experiments was captured by the harness program and streamed to a single text file.

The format chosen for the file of results was YAML, due to the ready availability of parsing libraries. YAML had an advantage over other formats such as CSV (comma-separated values) in that it allowed the flexibility of complex nested data were it to be necessary, while also being emitted in a streaming fashion. This greatly reduced the potential for output to be corrupted between algorithm executions.

\subsection{Results from the Experiments}
\label{subsec:results}

The automated suite of experiments was run numerous times over the course of this research. The final run from which the analysis and conclusions here are drawn is preserved in the same repository on the GitHub platform~\cite{github} as all the other files related to this research. The file of raw experiments data is named ``\texttt{experiments-data-20221111.yml}''. The final, full run of experiments took approximately 95 hours and 15 minutes to complete on the NUC device described in section~\ref{subsec:testing_platform}.

\subsubsection{Scope of the Experiments}

The final run of the experiments generated a total of 1,190 data-points taken from runs of the 30 programs. The set of programs below includes the regular expression variant of the DFA-Gap algorithm, as described in~\ref{subsubsec:dfa_regexp}. Table~\ref{table:iterations} shows the number of runs on a per-language, per-algorithm basis.

\begin{table}[h!]
\begin{center}
\input{tables/iterations}
\caption{Experiment iterations by language and algorithm}
\label{table:iterations}
\end{center}
\end{table}

For all numbers 10 and greater, there was an additional ``priming'' run (as described above) to ensure that disk cache status did not play into the values for full run-time or package-level energy usage. Note also that the DFA-Gap and Regexp-Gap columns are stand-ins for 5 such columns each (for the values of $k$ from 1 to 5). Each value of $k$ was run for the same number of iterations.

\subsubsection{Outliers and the Interpreted Languages}

While it was known that the interpreted languages (Python and Perl) would be slower than the compiled languages, the reality of the results was surprising: as will be shown in section~\ref{subsec:perf_comp}, below, the interpreted languages were in some cases more than 150 times slower than the fastest compiled program on the same algorithm.

This discovery required adjusting the automated experiments, to reduce the number of iterations for both interpreted languages. The Knuth-Morris-Pratt, Boyer-Moore, and Bitap algorithms were all reduced to 5 iterations each, for these languages. The Aho-Corasick algorithm ran in a more reasonable length of time and was left at 25 iterations, the same number as were run for the compiled languages.

For the DFA-Gap algorithm, both compiled and interpreted languages had to be reduced in terms of iterations given that approximate-matching algorithms are in general slower than their exact-matching counterparts. The compiled languages ran 5 iterations of this algorithm for values of $k$ ranging from 1 to 5, whereas the interpreted languages were necessarily limited to 3 iterations for the same values of $k$. When the regular expressions variant of the algorithm was added, it too was run for 5 and 3 iterations over the languages.

\subsection{Performance Comparisons}
\label{subsec:perf_comp}

The first of the three measures of the languages' suitability was chosen to be the overall performance. This was chosen for first consideration as this is the metric that most developers notice first: how long did it take the program to complete? Run-time measurement was also the easiest of the metrics to gather, both in terms of total program execution and in wall-clock time specifically spent in the algorithms themselves.

It was in the measuring of performance that it first became clear what a stark difference there was between the interpreted and compiled languages. One can forget the extent of this gap when working with interpreted languages in-depth for long periods of time. The Perl and Python performance numbers were so large in many cases that they had to be omitted from charts to preserve clarity, even when put to logarithmic scaling.

\subsubsection{Special Casing for Perl and Python}
\label{subsubsec:results:perf_perl_python}

Early examination of the performance results for Perl and Python (section~\ref{subsubsec:dfa_regexp}) had shown that the custom-built DFAs were strongly out-performed by using those languages' native support for regular expressions. Looking at these results separately from the rest of the experiments, the difference was considerable as is shown in figure~\ref{fig:graph:dfa_regexp_comp}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/dfa_regexp_comp.png}
	\caption{Bar charts of DFA vs. Regexp run-times for Perl and Python}
	\label{fig:graph:dfa_regexp_comp}
\end{figure}

Not only were the run-times themselves higher for the DFA implementations, but the growth in run-time as $k$ increases was more pronounced for the DFA implementations than for the regular expression implementations. Note that while Perl's DFA was outperformed by Python's for the first three values of $k$, at $k=4$ it was effectively equal and was slightly better at $k=5$. However, in the regular expression implementation Python remained the better performer consistently.

This had led to a significant question during development regarding the measurement of performance for the algorithms: should the Perl and Python manually-coded DFA implementations be replaced by the regular expression implementations? It is highly unlikely that an experienced programmer in either of these two languages would choose to create the DFA manually when they could instead create a regular expression based on the pattern and a given value of $k$, in any case. When the performance is so drastically disparate it seems even less likely.

It was decided instead to implement the regular expression version of the experiment for the remaining three languages and include this in the final results and analysis. Figure~\ref{fig:graph:dfa_regexp_comp2} shows the run-time differences for these languages.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{figures/dfa_regexp_comp2.png}
	\caption{Bar charts of DFA vs. Regexp run-times for the compiled languages}
	\label{fig:graph:dfa_regexp_comp2}
\end{figure}

In these charts, the C and C++ values are averaged from the three toolchains. Examination of the complete run-time tables for DFA-Gap (\ref{table:runtime:dfa_gap:combined}) and Regexp-Gap (\ref{table:runtime:regexp:combined}) showed that each toolchains' run-times were extremely close to each other. This is understandable, as the majority of the work in these cases would have been done by the PCRE2 library itself which is identical across the different toolchain executables.

Here the trend was the reverse of what had been seen in Perl and Python. The use of regular expressions by the compiled languages took more time than the hand-crafted DFAs had taken. Additionally, C++ showed to be faster than C when using regular expressions, and Rust faster still.

\subsubsection{Collected Performance Results}

After the decision was made on the regular expression question, processing of the full performance results was done. The collection of sub-tables in table~\ref{table:runtime:comparative-algorithm} shows the comparative performance of the languages on each of the algorithms. The time measurements are based on the algorithm run-time as opposed to the total run-time. In each table, the fastest language is listed first with the remaining ones following in order of performance. Run times are scaled by the fastest time. This has the result of showing each slower language's time as a percentage over the fastest.

\begin{table}[!htb]
\input{tables/runtimes}
\caption{Comparative run-times by algorithm}
\label{table:runtime:comparative-algorithm}
\end{table}

The DFA-Gap and Regexp algorithms are shown for just $k=3$. However, there was some fluctuation in the ranking of the languages as $k$ went from 1 to 5. For the application to the metric of performance, all values of $k$ for both of these algorithms will be used in the calculations.

The set of charts in figures~\ref{fig:graph:kmp-runtimes} to~\ref{fig:graph:regexp-runtimes} illustrate the run-times by language for each of the six algorithms. Note that all of these except figure~\ref{fig:graph:regexp-runtimes} omit the Perl and Python languages due to their extreme differences in the scale of running-time\footnote{As was mentioned, a logarithmic scale was also tried but the Perl/Python values were still too far from the norm (particularly in the Bitap results).}.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-kmp.png}
        \caption{KMP run-times}
        \label{fig:graph:kmp-runtimes}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-boyer_moore.png}
        \caption{BM run-times}
        \label{fig:graph:bm-runtimes}
    \end{minipage}
\end{figure}

Starting with the Knuth-Morris-Pratt (\ref{fig:graph:kmp-runtimes}) and Boyer-Moore (\ref{fig:graph:bm-runtimes}) algorithms which have a passing relation to each other, the red lines indicate the fastest run-time and are meant to show the difference in the other languages' finish-time. In the Knuth-Morris-Pratt results, of interest is the fact that both C and C++ compiled by the LLVM toolchain strongly out-performed the GCC and Intel toolchains. This would be the only algorithm in which LLVM controlled the performance for both languages. In Boyer-Moore, the Intel toolchain takes the top spot with C, and the second spot with C++. Though this is not the only algorithm for which a C++ experiment placed second, this is the highest that C++ would place throughout all algorithms.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-shift_or.png}
        \caption{Bitap run-times}
        \label{fig:graph:bitap-runtimes}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-aho_corasick.png}
        \caption{AC run-times}
        \label{fig:graph:ac-runtimes}
    \end{minipage}
\end{figure}

In the Bitap results shown in figure~\ref{fig:graph:bitap-runtimes}, the LLVM toolchain actually produced the slowest times within the C and C++ groups in this case. In the Aho-Corasick results (figure~\ref{fig:graph:ac-runtimes}) LLVM once again produced the fastest code with the C version of the experiment. The Aho-Corasick run-times also highlight the overall speed of this algorithm, powered by the ability to test all patterns in a single pass through each target sequence.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-dfa_gap.png}
        \caption{DFA run-times}
        \label{fig:graph:dfa-runtimes}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/algorithm_runtimes-regexp.png}
        \caption{Regexp run-times}
        \label{fig:graph:regexp-runtimes}
    \end{minipage}
\end{figure}

For the DFA and regular expression versions of the gap algorithm, figures~\ref{fig:graph:dfa-runtimes} and~\ref{fig:graph:regexp-runtimes}, a different style of charts was drawn. As both of these experiments ran for five different values of the $k$ parameter, the charts are designed to show all five run-times for each language as a stack.

The DFA experiments follow an almost expected trajectory, with C being faster than C++ and C++ faster than Rust. However, the regular expression experiments show an exactly opposite trend, with Rust being the fastest of the languages. Figure~\ref{fig:graph:regexp-runtimes} also includes results for the Perl and Python languages, as this experiment gave results for them that could be comfortably included.

Full tables of run-times scores for all values of $k$ in both DFA-Gap and Regexp-Gap experiments can be found in section~\ref{sec:extra_tables}.

\subsection{Expressiveness Comparisons}
\label{subsec:expr_comp}

Expressiveness was generally the most-challenging of the metrics to measure and evaluate. Even the most concrete, numerical measurements of code such as SLOC and complexity are subject to some debate and dissent. In these aspects of the research, measurement was taken through a range of open-source tools and the results tracked closely with expectation; the interpreted languages showed the strongest scores on the three expressiveness sub-metrics.

However, much of what was measured for this metric was also dependent upon the skill shown in the writing of the programs. Familiarity with the different languages varied, from significant experience in C and Perl to relative newness with regards to Rust. It is not possible to say whether someone more expert in the Rust language, for example, could have implemented the chosen algorithms with more efficiency and better expressiveness metrics.

Additionally, the complexity component of this metric had been cast in some doubt by the inability to use a single tool for all language involved. This is shown in section~\ref{subsec:combined} to have not influenced the final rankings.

In comparing expressiveness, it is useful to look at all three of the chosen comparative measures in an aggregated fashion. First, the individual numbers will be examined. As with the previous two bases, tables will show the numbers comparing the languages. Unlike the previous sections, the numbers shown will be for the combined files of all algorithm modules, as well as runner modules and input modules. For each language the source code was combined in a way that facilitated each of the metrics:

\begin{description}
\item[SLOC:] Total SLOC values for each language's files were combined. In the case of C and C++, this includes relevant lines from the header files for the runner and input modules.
\item[Complexity:] Each file's cyclomatic complexity values were computed on a per-function basis. Any function whose value was exactly 1 was discarded so as to not artificially lower the average complexity for the module. Each module's function-level scores were then averaged, and all modules' averages for a given language were summed together.
\item[Conciseness:] Using a method adapted from Bergmans, et al~\cite{bergmans}, each source code file for a given language was stripped of comments and then merged into a sort of ``archive'' using the standard ``\texttt{cat}'' command available on Linux. Each such resulting file was then compressed with the ``\texttt{xz}'' compression utility and the compression ratio recorded.
\end{description}

\subsubsection{Source Lines of Code}

In table~\ref{table:expr:sloc}, the SLOC totals are shown in three sub-tables: only the algorithm implementations, then the framework totals, and finally the combined totals of all lines.

\begin{table}[!htb]
\input{tables/sloc}
\caption{Comparison of SLOC by language}
\label{table:expr:sloc}
\end{table}

Here Python is the clear leader, with Perl being 40\% larger in table~\ref{table:sloc:all}. C++ maintains the third-place ranking across all three sub-tables, and Rust edges out C in both the framework measurement and the total lines measurement. Rust's score in table~\ref{table:sloc:algorithm} likely suffered from the necessary mechanism that was used to pass and unpack pattern representation between a given algorithm's initialization function and the algorithm function itself. Compare the listings, below.

\input{code/unpack.c}

\input{code/unpack.rs}

In this area, C and C++ each needed only one line per element passed (listing~\ref{lst:c_unpack}), while Rust required four\footnote{One of which was a closing-brace with a semicolon and might not have been counted by the \texttt{sloc} tool.} (listing~\ref{lst:rust_unpack}). This is a 12-line difference in just the Aho-Corasick implementation, so it can be understood how this would likely propagate through the other algorithm implementations.

By further comparison: both Python and Perl, having the native ability to pass arrays of differing types, needed only one line to unpack the pattern representations.

\subsubsection{Cyclomatic Complexity}
\label{subsubsec:complexity}

Table~\ref{table:expr:cyclomatic} shows a similar break-down of the cyclomatic complexity measurements: algorithms, framework, and combined total.

\begin{table}[!htb]
\input{tables/cyclomatic}
\caption{Comparison of complexity by language}
\label{table:expr:cyclomatic}
\end{table}

Unlike the SLOC measurements, however, there is greater variation in the rankings of the languages. Here the contest between the first and second rankings was between Python and C++. Rust placed last in the measure of the algorithm code, possibly due to the same issue of excess lines around the unpacking of pattern data. Rust did rank third in the framework table~\ref{table:cyclomatic:framework} where it also had a significantly lower average value than C++ or Python. Even as Rust ended tied for the fourth rank in the total table, its totaled average complexity was still the lowest of the five.

It is important here to note again that it was necessary to use different tools to calculate the complexity of the Perl and Rust code. The \texttt{lizard} tool did not support Perl at all, and examination of the Rust results showed some bugs in the handling of Rust. Because of this, it is not possible to say with certainty that the relative comparisons of complexity between the five languages are completely sound. For the purposes of this paper the total complexity values will be used as presented here. It is felt that this will be sufficient as these values will only contribute $\frac{1}{3}$ of the value of the final expressiveness value.

\subsubsection{Conciseness}
\label{subsubsec:conciseness}

Conciseness proved to be a difficult concept to quantify. The approach taken in~\cite{bergmans} was designed around a significantly larger database of source code, but as the results from that paper were compelling, it led to the adaptation of a variation of that methodology here.

The steps for deriving this measurement were:

\begin{enumerate}
\item A clean copy of all source code was made into a series of directories named by language
\item All comments and blank lines were removed by the \texttt{cloc} tool
\item Each individual language directory had all text files concatenated into a single file
\item Each of these text files were compressed with \texttt{xv} and the compression ratio recorded
\end{enumerate}

A key difference between~\cite{bergmans} and here is the limited scope of the data being analyzed; since there is essentially just one ``project'' being put to scrutiny, the results are sensitive to the fact that some of the files were smaller in size to begin with. Initially the technique in~\cite{bergmans} was followed exactly, and involved using the standard \texttt{tar} utility to create the archives of each language. But when put into practice it was found that the metadata overhead of a \texttt{tar} archive was more than the size of the actual data in some cases.

Further, in their paper, Bergmans and their team had removed most smaller examples of code for each language from the final measurements. This was not an option here given the limited size and number of the samples. For this reason the decision was made to focus the compression on the text content only, by applying concatenation to the stripped versions of the source files and compressing the results of this process.

In table~\ref{table:expr:compression}, the results are shown.

\begin{table}[!htb]
\input{tables/compression}
\caption{Comparison of compressibility by language}
\label{table:expr:compression}
\end{table}

The ranking of Python and Perl as the first two is expected. However, the placing of C ahead of both Rust and C++ came as a surprise given the presence of highly-repetitive calls to library routines for the manual memory management. However, C and Rust both likely benefited from the need for additional support code in the Aho-Corasick algorithm which could have brought the compression ratio slightly further down. Lastly, as with the previous two metrics, it is likely that Rust was effectively penalized for the very verbose and detailed blocks that were necessary for passing the processed pattern data between initialization and algorithm functions.

\subsubsection{Combining the Expressiveness Metrics}

At this point the next step was to derive a single measurement of expressiveness from the three measurements taken. For this, it would be needed to have ``score'' values for cyclomatic complexity that were in line with the scores computed for SLOC and conciseness. These values were computed from table~\ref{table:cyclomatic:total} and are shown in table~\ref{table:expr:cyclomatic-score}.

\begin{table}[!htb]
\input{tables/cyclomatic-score}
\caption{Scoring of the cyclomatic complexity results}
\label{table:expr:cyclomatic-score}
\end{table}

Here, the score shows a virtual tie for first position between C++ and Python. Their gap is just one point (less than 1\%), while the next gap is 43 (an increase of almost 35\%).

Now it would be possible to utilize the three expressiveness metrics to calculate a single value for each language. The following values would be combined into a series of 3-element vectors for each language:

\begin{itemize}
\item The SLOC scores from table~\ref{table:sloc:all}, the total of lines
\item The cyclomatic scores from table~\ref{table:expr:cyclomatic-score}
\item The conciseness scores from table~\ref{table:expr:compression}
\end{itemize}

The vectors were normalized into unit vectors. Then the vectors' lengths were calculated and scaled by the lowest value, giving the values in table~\ref{table:expr:expressiveness-score}:

\begin{table}[!htb]
\input{tables/expressiveness}
\caption{Calculated expressiveness score}
\label{table:expr:expressiveness-score}
\end{table}

It was not a surprise that Python ended up ranked as the most-expressive of the five contenders, as it had led the group in all three expressiveness metrics. The ranking of C++ ahead of Perl by just over 5 percentage points was more of a surprise, while the presence of C at the bottom was not surprising.

Figure~\ref{fig:graph:expressiveness-arrows} illustrates the expressiveness values as vectors anchored at the origin. Judging the length of the vectors helps to show the difference between Python in the \nth{1} position and C in the \nth{5}.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.85\textwidth]{figures/expressiveness_arrows.png}
    \caption{Calculated expressiveness as vectors}
    \label{fig:graph:expressiveness-arrows}
\end{figure}

Because of the possibly-inconsistent nature of the complexity data, it was decided to derive a second score based on just the SLOC and compression scores. All other calculation steps were the same. This resulted in table~\ref{table:expr:expressiveness-score2}.

\begin{table}[!hb]
\input{tables/expressiveness2}
\caption{Calculated expressiveness score, 2-axis}
\label{table:expr:expressiveness-score2}
\end{table}

Here, the results more closely tracked with expectations: Python and Perl head the list while C++ and Rust take the next two places. As has been mentioned, it is possible that Rust suffered in the measurements due to the data-unpacking mechanism required by the framework.

\subsection{Energy Usage Comparisons}
\label{subsec:energy_comp}

In this section the energy usage results are examined. Table~\ref{table:energy:comparative-algorithm} shows the comparative energy usage over time (Joules per second) for each algorithm with the same scaling methodology applied as was used for the performance tables. The tables here use the ``Package'' and ``DRAM'' energy values combined together, divided by total run-time.

In regards to energy usage, the only significant barrier to overcome was ensuring that the machine used to run the experiments would be sufficiently isolated so as to have the least amount of interference possible from other running processes. The nature of the RAPL system of measuring power consumption also required that the experiments be run with super-user access levels. This amplified the need for the code to be as safe as possible, even though the testing machine was not made accessible to the general internet.

RAPL posed other challenges to use, such as the previously-mentioned 32-bit limitation in the registers used for tracking the energy usage. But this was overcome, and the RAPL system ended up proving robust-enough to handle measuring programs whose run-times ranged from barely one second to nearly three hours.

\begin{table}[!htb]
\input{tables/energy}
\caption{Comparative energy usage over time by algorithm}
\label{table:energy:comparative-algorithm}
\end{table}

The language exhibiting the lowest energy usage is listed first, with the rest ranked behind it. The DFA-Gap and Regexp algorithms are again represented by $k=3$, and again the rankings of the languages varied with $k$.

These results are further illustrated by figure~\ref{fig:graph:power_per_sec}. In this collection of bar-charts, the Rust language (represented by the pink bars) can be seen to be the lowest value in the Knuth-Morris-Pratt, Boyer-Moore, and DFA-Gap instances. In fact, Rust scored the lowest energy usage for all five variations of the DFA-Gap algorithm, making it the most-efficient language for 7 of the 14 distinct groups of experiments.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.85\textwidth]{figures/power_per_sec.png}
    \caption{Energy/second, by algorithm}
    \label{fig:graph:power_per_sec}
\end{figure}

The fact that Rust was the overall best performer in terms of energy usage is underscored by the chart in figure~\ref{fig:graph:total_power_usage}. In this chart, the total (mean) energy usage by all experiments in each language is shown. Here, the Perl and Python languages have been removed again due to the fact that their longer run-times lead to energy numbers well out of scale with respect to the compiled languages. The red horizontal line shows the total usage by Rust and illustrates the differences with C and C++ across all three of their toolchains.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.85\textwidth]{figures/total_power_usage.png}
    \caption{Total energy by language}
    \label{fig:graph:total_power_usage}
\end{figure}

As with the run-times score tables, full tables of energy scores for all values of $k$ in both DFA-Gap and Regexp-Gap experiments can be found in section~\ref{sec:extra_tables}.

\subsection{Combining the Bases}
\label{subsec:combined}

There were now three distinct measurements available for combination and analysis. The first step was to derive a single set of scores for the run-time basis and the energy-used basis, combining the values from all 14 groups of experiments into single values. For this, values were simply added together then scaled by the minimum value as has been done elsewhere. The total run-time table~\ref{table:total:runtime} shows these values, derived from the algorithm-only run-time metric. In table~\ref{table:total:energy}, the same is done with the combination of the ``Package'' and ``DRAM'' energy metrics.

\begin{table}[!htb]
\input{tables/runtime_energy_totals}
\caption{Final scores for totaled run-time and energy, by language}
\label{table:totals:runtime-energy}
\end{table}

From here came a big question in two parts: Should the final scoring be based on the languages' scores for these bases or based on the language rankings? And secondly, should the cyclomatic complexity data be included in the expressiveness basis or not?

While the question regarding the complexity data has already been discussed, it still remained to be seen whether it would affect the final rankings. More than this, however, was the question of whether to simply apply the ranking of each language to determine final placement. Doing so might create a stronger distinction between adjacent languages than the scores themselves would. On the other hand, using the scores would retain the subtlety in the differences between languages.

The decision was made to examine all combinations of these two variables. Table~\ref{table:totals:all_metrics} shows the four sub-tables that result from this analysis. As can be seen from comparing \ref{table:final:scale_and_cc} with \ref{table:final:scale_no_cc}, and \ref{table:final:rank_and_cc} with \ref{table:final:rank_no_cc}, the cyclomatic complexity values did not affect the respective rankings at all: both pairs of tables have identical placement for their 9 rows. Comparing column-wise (\ref{table:final:scale_and_cc} with \ref{table:final:rank_and_cc}, and \ref{table:final:scale_no_cc} with \ref{table:final:rank_no_cc}), it can be seen that using the rank in place of scores lead to a slight difference in final ranking. Specifically, Rust and Python each moved up one place under a ranking basis. More significant, though, is how the different approaches affected the changes in scores.

\begin{table}[!htb]
\input{tables/final_scores}
\caption{Final scores for all combined metrics, by language}
\label{table:totals:all_metrics}
\end{table}

Comparing the two sub-tables that include complexity values, it can be seen that the difference between the top (GCC C++) and bottom (Perl) is slightly higher in \ref{table:final:scale_and_cc} than in \ref{table:final:rank_and_cc}. This difference is even more pronounced in the two non-complexity tables. In addition to this, the growth of the score value is smoother when the languages are arranged based on rank. This is illustrated in figure~\ref{fig:graph:final_scores_plot}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/final_scores_plot.png}
	\caption{Plot of the rise in final score by ranking}
	\label{fig:graph:final_scores_plot}
\end{figure}

In the lines for the score-based ordering (marked by solid circles on the plot), the final scores remain well under 1.5 until the \nth{8} and \nth{9} positions (which correspond to Python and Perl respectively). This is due to the fact that score-based ordering would necessarily be very reactive to the significant jumps in the final scores for run-time and energy usage. Looking at the lines for ranking-based ordering (solid triangles), these follow a more linear path while the final scores for Perl still ending up in the same general range as for the score-based ordering.

The lines for score-based ordering also show the effect of the top three entries' scores being within a range of 0.07\%, resulting in flat lines until the \nth{4} data-point. The ranking-based ordering exhibits this same pattern for the first two data-points as well, due to the entries in \nth{1} and \nth{2} positions having identical expressiveness rankings and reciprocal rankings for run-time and energy.

Throughout these experiments, the C and C++ languages have been evaluated for three different toolchains each. What would the final results look like, if each of C and C++ were represented by only their best-scoring variants?

\begin{table}[!htb]
\input{tables/final_distinct}
\caption{Final scores for all combined metrics, by distinct language}
\label{table:totals:all_metrics_distinct}
\end{table}

In table~\ref{table:totals:all_metrics_distinct} the five distinct languages are shown with their ordering and score based on ranking, both with and without the complexity metrics. Now, without C and C++ creating ``ties'' in the application of expressiveness ranks to the remaining two ranks, the Rust language comes out on top. This is keeping with the fact that Rust had held the top spot for both run-times and energy usage.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/final_distinct.png}
	\caption{Plot of the rise in final score, using distinct languages}
	\label{fig:graph:final_distinct_plot}
\end{figure}

In figure~\ref{fig:graph:final_distinct_plot}, the line for the first four rankings is much more straight than had been in figure~\ref{fig:graph:final_scores_plot}. It is worth noting that the change between Python in \nth{4} place and Perl in \nth{5} is much smaller than for the previous places.
